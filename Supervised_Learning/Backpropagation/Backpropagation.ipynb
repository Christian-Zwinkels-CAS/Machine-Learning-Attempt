{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "This document will explain the code I used to attempt to implement the backpropagation machine learning algorithm as well as an attempt to explain my understainding of it. Later on once I have more experience I will reflect on this.\n",
    "\n",
    "## What is it?\n",
    "Backpropagation is an algorithm that is used in neural networks to learn. It is a form of supervised learning where the user gives it training examples with features and a desired output. The machine can try to predict the output then it will use a mutivariable function in order to calculate the error of each weight and bias. It does this for each training example then it tries to find the minimum of this function (essentially finding out what weights and biases correspond to the lowest error) by using gradient descent. The algorithm starts at the last layer then it goes back one layer at a time to the last layer which is why its called backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "the dataset I used was a simple one which is a XOR gate. It has two inputs and each can either be one or a zero. The output will be zero if both inputs are zeros or ones.\n",
    "\n",
    "|Input|Output|\n",
    "|-----|------|\n",
    "|0, 0 |   0  |\n",
    "|0, 1 |   1  |\n",
    "|1, 0 |   1  |\n",
    "|1, 1 |   0  |\n",
    "\n",
    "This dataset is small which means that during testing I dont have to wait long periods of time just to see how well the program did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Neural Network\n",
    "The architecture of the neural network was simple, but I wanted my program to be able to use any size of neural network. In this example I used one with an input layer, one hidden layer, and the output layer which has only one output.\n",
    "![Alt Text](https://raw.githubusercontent.com/Christian-Zwinkels-CAS/Machine-Learning-Attempt/master/Supervised_Learning/Backpropagation/xor_2-Copy.png)\n",
    "W1 and W2 are sets of weights represented as matricies: $$ W^{(1)} = \\begin{bmatrix}\n",
    "w_{1} &w_{2}\\\\ \n",
    "w_{3} &w_{4} \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "$$ W^{(2)} = \\begin{bmatrix}\n",
    "w_{5} &w_{6}\n",
    "\\end{bmatrix} $$\n",
    "## How it predicts\n",
    "The neural network is given a trainig exaple say 0, 0 to get the hidden layers. Lets see for example how it calculates the value at h1: $$ x1 * w_{1} + x2 * w_{2} + b^{(1)} $$\n",
    "Where b is a real number called the bias which is in each perceptron.\n",
    "We can calculate the value of x1 and x2 simultaneously by using matrices: $$ \\begin{bmatrix}\n",
    "h1\\\\ \n",
    "h2\n",
    "\\end{bmatrix} = \\sigma \\left ( \\begin{bmatrix}\n",
    "w_{1} &w_{2}\\\\ \n",
    "w_{3} &w_{4} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x1\\\\ \n",
    "x2\n",
    "\\end{bmatrix} + b^{(1)}\\right ) $$\n",
    "And for the final output layer: $$ y = \\sigma \\left ( \\begin{bmatrix}\n",
    "w5\\\\ \n",
    "w6\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "h1\\\\ \n",
    "h2\n",
    "\\end{bmatrix} + b^{(2)}\\right) $$\n",
    "$ \\sigma $ is the activation function which in this case is the sigmoid funciton. The activation function takes an input and turns it into a value between zero and one. $$ \\sigma (x) = \\frac{1}{1+e^{-x}} $$\n",
    "Initially the weights are random values and the biases are zeros, but after training these values would have changed. This whole process is called the forward pass.\n",
    "## How it learns\n",
    "Backpropagation uses a technique called gradient descent where it tries to find the minimum of a function that takes all the parameters in order to calculate the error called the cost function.\n",
    "### The cost function\n",
    "Firstly the machine needs to know how wrong it was, and to do that an error function is defined. Let $ x^{(i)} $ be the i<sup>th</sup> training example, $ y^{(i)} $ be the training examples's desired output, and $ \\hat{y}^{(i)} $ be the machine's prediction. We define the error of that training example to be the squared error: $$ C(x^{(i)}) = (\\hat{y}^{(i)} -  y^{(i)})^{2}$$\n",
    "We want to know how well it did over all the training examples so we take the average squared error for each training example letting $ m $ equal the number of training examples letting $ W $ be the set of all the weights: $$ C(W) = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}^{(i)}-y^{(i)})^{2} $$\n",
    "### Gradient descent\n",
    "We can not plot the cost function due to it being a multivariable function with the number of inputs being the number of weights and biases, but I will show a more simplified example in order to give an intuition of whats going on. I will later link a video that shows a really nice animation and explanation to what is going on.\n",
    "\n",
    "A simple cost funtion with just one parameter is a parabola:\n",
    "![Cost function simple graph placeholder](Screenshot_1.png)\n",
    "We start at a random value of $ w $ and the goal is to minimize the cost so we need to tell the computer to change $ w $ in a way that it will get closer to the minimum point of the graph. The negative of the derivative of the graph corresponds to lowering the value of the cost funtion so we take the derivative of the cost function with respect to $ w $ \n",
    "\n",
    "If we have two parameters the simplified version of the cost funtion will be a paraboloid so instead of taking a derivative we have to take a partial derivative for every weight.\n",
    "\n",
    "To compute this derivative we have to use the chain rule. This is where I will let the video do the explaining as it really hepls having animations and writing all the process will make this document really long.\n",
    "[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/tIeHLnjs5U8/0.jpg)](http://www.youtube.com/watch?v=tIeHLnjs5U8)\n",
    "This process repeates itself for the amount of iterations specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "I used python due to its simple syntax and popularity meaning that there are a lot of resources to help me. The only module I used was NumPy. I wanted to make this as from scratch as possible.\n",
    "\n",
    "First I imported Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I defined the activation function and its derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made the dataset (XOR gate truth table):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = np.array([[0, 0],\n",
    "                    [0, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1]])\n",
    "\n",
    "data_out = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialized the architechture of the weights making sure to use loops so I can change the architecture easily. The weights started out as random values from the normal distribution and the biases as zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = (2, 2, 1)\n",
    "weight_sizes = [(y, x) for y, x in zip(layer_sizes[1:], layer_sizes[:-1])]\n",
    "weights = [np.random.standard_normal(s) for s in weight_sizes]\n",
    "biases = [np.zeros((s, 1)) for s in layer_sizes[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook the code will be a bit different from now on as I want it to be organized a bit better, I make a method to fowardpropagate a sample which stores all the layers in a list of numpy arrays which it then returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(X):\n",
    "    a = X.reshape((layer_sizes[0], 1))  # Turns it into a column matrix\n",
    "    z = a\n",
    "    layers = []\n",
    "    for w, b in zip(weights, biases):\n",
    "        z = np.dot(w, z) + b\n",
    "        layers.append(z)\n",
    "    layers_sigmoid = [sigmoid(i) for i in layers]\n",
    "    layers_sigmoid.insert(0, a)  # Adds the activation layers to the begging\n",
    "    return layers_sigmoid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
