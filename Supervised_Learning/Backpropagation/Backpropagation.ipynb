{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "This document will explain the code I used to attempt to implement the backpropagation machine learning algorithm as well as an attempt to explain my understainding of it. Later on once I have more experience I will reflect on this.\n",
    "\n",
    "## What is it?\n",
    "Backpropagation is an algorithm that is used in neural networks to learn. It is a form of supervised learning where the user gives it training examples with features and a desired output. The machine can try to predict the output then it will use a mutivariable function in order to calculate the error of each weight and bias. It does this for each training example then it tries to find the minimum of this function (essentially finding out what weights and biases correspond to the lowest error) by using gradient descent. The algorithm starts at the last layer then it goes back one layer at a time to the last layer which is why its called backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "the dataset I used was a simple one which is a XOR gate. It has two inputs and each can either be one or a zero. The output will be zero if both inputs are zeros or ones.\n",
    "\n",
    "|Input|Output|\n",
    "|-----|------|\n",
    "|0, 0 |   0  |\n",
    "|0, 1 |   1  |\n",
    "|1, 0 |   1  |\n",
    "|1, 1 |   0  |\n",
    "\n",
    "This dataset is small which means that during testing I dont have to wait long periods of time just to see how well the program did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Neural Network\n",
    "The architecture of the neural network was simple, but I wanted my program to be able to use any size of neural network. In this example I used one with an input layer, one hidden layer, and the output layer which has only one output.\n",
    "![Alt Text](https://raw.githubusercontent.com/Christian-Zwinkels-CAS/Machine-Learning-Attempt/master/Supervised_Learning/Backpropagation/xor_2-Copy.png)\n",
    "W1 and W2 are sets of weights represented as matricies: $$ W^{(1)} = \\begin{bmatrix}\n",
    "w_{1} &w_{2}\\\\ \n",
    "w_{3} &w_{4} \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "$$ W^{(2)} = \\begin{bmatrix}\n",
    "w_{5} &w_{6}\n",
    "\\end{bmatrix} $$\n",
    "## How it predicts\n",
    "The neural network is given a trainig exaple say 0, 0 to get the hidden layers. Lets see for example how it calculates the value at h1: $$ x1 * w_{1} + x2 * w_{2} + b^{(1)} $$\n",
    "Where b is a real number called the bias which is in each perceptron.\n",
    "We can calculate the value of x1 and x2 simultaneously by using matrices: $$ \\begin{bmatrix}\n",
    "h1\\\\ \n",
    "h2\n",
    "\\end{bmatrix} = \\sigma \\left ( \\begin{bmatrix}\n",
    "w_{1} &w_{2}\\\\ \n",
    "w_{3} &w_{4} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x1\\\\ \n",
    "x2\n",
    "\\end{bmatrix} + b^{(1)}\\right ) $$\n",
    "And for the final output layer: $$ y = \\sigma \\left ( \\begin{bmatrix}\n",
    "w5\\\\ \n",
    "w6\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "h1\\\\ \n",
    "h2\n",
    "\\end{bmatrix} + b^{(2)}\\right) $$\n",
    "$ \\sigma $ is the activation function which in this case is the sigmoid funciton. The activation function takes an input and turns it into a value between zero and one. $$ \\sigma (x) = \\frac{1}{1+e^{-x}} $$\n",
    "Initially the weights are random values and the biases are zeros, but after training these values would have changed. This whole process is called the forward pass.\n",
    "## How it learns\n",
    "Backpropagation uses a technique called gradient descent where it tries to find the minimum of a function that takes all the parameters in order to calculate the error called the cost function.\n",
    "### The cost function\n",
    "Firstly the machine needs to know how wrong it was, and to do that an error function is defined. Let $ x^{(i)} $ be the i<sup>th</sup> training example, $ y^{(i)} $ be the training examples's desired output, and $ \\hat{y}^{(i)} $ be the machine's prediction. We define the error of that training example to be the squared error: $$ C(x^{(i)}) = (\\hat{y}^{(i)} -  y^{(i)})^{2}$$\n",
    "We want to know how well it did over all the training examples so we take the average squared error for each training example letting $ m $ equal the number of training examples: $$ \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}^{(i)}-y^{(i)})^{2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
