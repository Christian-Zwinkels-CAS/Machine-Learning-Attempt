{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Network Using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is Google Brain Teams's open source libary for machine learning. Here I will code a convolutional neural network to classify the fashion MNIST dataset as the regular MNIST dataset has been done too many times. I decided to use a convolutional neural network as it is better for working with images. Convolutional neural networks use filters in order to detect features that can help with the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip as gz\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset I used contained 60000 images of 10 different types of fashion items such as clothes and shoes an can be found at: https://github.com/zalandoresearch/fashion-mnist with each image labeled with what kind of fashion item it is. It is the same as the MNIST dataset but with fashion instead. It uses the exact same format as the MNIST dataset just with different images. First the files are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = gz.open(\"Data/train-images-idx3-ubyte.gz\", \"r\")\n",
    "train_y = gz.open(\"Data/train-labels-idx1-ubyte.gz\", \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the size of the images (which are 28x28) and the number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 28\n",
    "num_images = 60000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in a sequence of bytes. The first 16 bytes are read but not assigned any value since they do not contain any usable data (as said in the original MNIST dataset website: http://yann.lecun.com/exdb/mnist/) then it reads the data in such a way that every training example is separated. Finally it reshapes it into a usable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.read(16)  # First 16 bytes contain unecessary information\n",
    "train_x = train_x.read(img_size * img_size * num_images)\n",
    "train_x = np.frombuffer(train_x, dtype=np.uint8())\n",
    "train_x = train_x.reshape(num_images, img_size, img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same concept for the labels but it is converted to a list instead of a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.read(8)  # First 8 bytes contain unecessary info\n",
    "train_y = train_y.read(num_images)\n",
    "train_y = np.frombuffer(train_y, dtype=np.uint8)\n",
    "train_y = train_y.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here each label is assinged a row (but idealy should be row) matrix with all 0s expect for one element. Each element shows the probability of it being that label so for example a picture of a pullover is labeled as a 2 (labels range from 0 to 9) then the third element (python starts counting from 0 so the first element is indexed as teh 0th element) is going to be a 1.\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "P(Shirt) \\\\\n",
    "P(Trouser) \\\\\n",
    "P(Pullover \\\\\n",
    "\\vdots \\\\\n",
    "P(Ankle boot)\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_y)):\n",
    "    index = train_y[i]\n",
    "    train_y[i] = np.zeros((1, 10), dtype=np.float32)\n",
    "    np.put(train_y[i], index, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the data is saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Data/train_features.npy\", train_x)\n",
    "np.save(\"Data/train_labels.npy\", train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image data can be visualy shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tf.squeeze(train_x[np.random.randint(0, len(train_y))]), cmap=\"binary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the data is loaded and converted to tensors. A new axis is added to repressent the amount of colour channels so the dimension in general are: width $\\times$ height $\\times$ number of colour channels. The data is shuffled which makes the machine learn to fit all the data not just the first types, and batched so that I don't get an out of memory issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.load(\"Data/train_features.npy\")\n",
    "train_y = np.load(\"Data/train_labels.npy\")\n",
    "# c = list(zip(train_x, train_y))\n",
    "# random.shuffle(c)\n",
    "# train_x, train_y = zip(*c)\n",
    "train_x = tf.convert_to_tensor(train_x, dtype=tf.float32)\n",
    "train_x = train_x / 255.0\n",
    "train_x = train_x[..., tf.newaxis]\n",
    "train_y = tf.convert_to_tensor(train_y, dtype=tf.float32)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(1000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(TrainModel, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(16, 3, activation=\"relu\")\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d1 = tf.keras.layers.Dense(128, activation=\"relu\")\n",
    "        self.d2 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.d3 = tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pool1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        return self.d3(x)\n",
    "\n",
    "model = TrainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "accuracy = tf.keras.metrics.Accuracy()\n",
    "train_loss = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function  # This just increases performance\n",
    "def TrainStep(data_in, data_out):\n",
    "    with tf.GradientTape() as g:\n",
    "        preds = model(data_in)\n",
    "        loss_funct = loss(data_out, preds)\n",
    "    gradients = g.gradient(loss_funct, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    accuracy(tf.squeeze(data_out), preds)\n",
    "    train_loss(loss_funct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        accuracy.reset_states()\n",
    "        train_loss.reset_states()\n",
    "\n",
    "        for img, labels in dataset:\n",
    "            TrainStep(img, labels)\n",
    "\n",
    "        placeholder = \"Epoch: {}, Loss: {}, Accuracy: {} %\"\n",
    "        print(placeholder.format(epoch + 1, train_loss.result(), np.around(accuracy.result()*100, decimals=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just messing with tensorflows gradient tape\n",
    "x = tf.constant(5.0)\n",
    "with tf.GradientTape() as g:\n",
    "  g.watch(x)\n",
    "  y = 2*x**2 + 5*x\n",
    "dy_dx = g.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=25.0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.010215820744633675, Accuracy: 28.5 %\n",
      "Epoch: 2, Loss: 0.01064203679561615, Accuracy: 28.200000762939453 %\n",
      "Epoch: 3, Loss: 0.011628749780356884, Accuracy: 28.299999237060547 %\n",
      "Epoch: 4, Loss: 0.011766750365495682, Accuracy: 29.799999237060547 %\n",
      "Epoch: 5, Loss: 0.010643944144248962, Accuracy: 30.700000762939453 %\n",
      "Epoch: 6, Loss: 0.013059194199740887, Accuracy: 29.0 %\n",
      "Epoch: 7, Loss: 0.010710659436881542, Accuracy: 29.799999237060547 %\n",
      "Epoch: 8, Loss: 0.011056344024837017, Accuracy: 31.200000762939453 %\n",
      "Epoch: 9, Loss: 0.01208242867141962, Accuracy: 31.799999237060547 %\n",
      "Epoch: 10, Loss: 0.009612374007701874, Accuracy: 31.600000381469727 %\n",
      "Epoch: 11, Loss: 0.006708370056003332, Accuracy: 30.799999237060547 %\n",
      "Epoch: 12, Loss: 0.013408802449703217, Accuracy: 31.299999237060547 %\n",
      "Epoch: 13, Loss: 0.009910215623676777, Accuracy: 32.29999923706055 %\n",
      "Epoch: 14, Loss: 0.008611105382442474, Accuracy: 31.899999618530273 %\n",
      "Epoch: 15, Loss: 0.008056191727519035, Accuracy: 32.20000076293945 %\n"
     ]
    }
   ],
   "source": [
    "train(train_ds, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\chris\\.conda\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: Fashion.model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Fashion.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
